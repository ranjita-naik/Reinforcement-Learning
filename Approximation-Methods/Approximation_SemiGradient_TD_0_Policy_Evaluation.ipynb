{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "POSSIBLE_ACTIONS = ('U','D','L','R') \n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Environment\n",
    "class Grid:\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.i = state[0]\n",
    "        self.j = state[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return(self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions \n",
    "    \n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action =='D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1 \n",
    "        \n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "        \n",
    "    \n",
    "    def undo_move(self):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action =='D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1 \n",
    "        \n",
    "        assert(self.current_state() in self.all_states())\n",
    "                \n",
    "            \n",
    "    def game_over(self):\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    \n",
    "    def all_states(self):\n",
    "        return set().union(self.actions.keys(), self.rewards.keys())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    # .  .  .  1\n",
    "    # .  x  .  -1\n",
    "    # s  .  .  .\n",
    "    \n",
    "    # 1, -1 are rewards for the corresponding positions\n",
    "    # 'x'  not allowed\n",
    "    # 's'  start state\n",
    "    grid = Grid(3, 4, (2,0))\n",
    "    rewards = {(0,3) : 1, (1,3) : -1}\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    grid.set(rewards, actions)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = np.random.randn(4) \n",
    "\n",
    "  \n",
    "    def states_to_feature(self, s):\n",
    "        return np.array([s[0] , s[1] , s[0] * s[1] , 1])\n",
    "\n",
    "\n",
    "    def predict(self, s):\n",
    "        x = self.states_to_feature(s)\n",
    "        return self.theta.dot(x)\n",
    "\n",
    "    def grad(self, s):\n",
    "        return self.states_to_feature(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_values(V, grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(P, grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < eps:\n",
    "        return np.random.choice(POSSIBLE_ACTIONS)\n",
    "    else:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play_grid_world(grid, policy):\n",
    "    s = (2,0)\n",
    "    grid.set_state(s)\n",
    "    states_and_rewards = [(s,0)]\n",
    "    \n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        a = random_action(a)\n",
    "        r = grid.move(a)\n",
    "        states_and_rewards.append((s,r))\n",
    "        s = grid.current_state()\n",
    "    states_and_rewards.append((s,r))\n",
    "    return states_and_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "\n",
      "\n",
      "values\n",
      "---------------------------\n",
      " 0.77| 0.86| 0.96| 0.00|\n",
      "---------------------------\n",
      " 0.64| 0.00|-0.03| 0.00|\n",
      "---------------------------\n",
      " 0.50|-0.25|-1.01|-1.77|\n",
      "\n",
      "\n",
      "policy\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    grid = standard_grid()\n",
    "    \n",
    "    print(\"Rewards\")\n",
    "    print_values(grid.rewards, grid)\n",
    "    \n",
    "    \n",
    "    # policy\n",
    "    policy = {\n",
    "        (0,0):'R',\n",
    "        (1,0):'U',\n",
    "        (2,0):'U',\n",
    "        (0,1):'R',\n",
    "        (1,2):'R',\n",
    "        (2,1):'R',\n",
    "        (0,2):'R',\n",
    "        (2,2):'R',\n",
    "        (2,3):'U',\n",
    "    }\n",
    "    \n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "            \n",
    "    model = Model()\n",
    "    deltas = []\n",
    "    t = 1.0\n",
    "    for e in range(30000):\n",
    "        if e % 100 == 0:\n",
    "            t += 0.01\n",
    "            alpha = LEARNING_RATE/t\n",
    "            \n",
    "        max_change = 0\n",
    "        # play an episode\n",
    "        states_and_rewards = play_grid_world(grid, policy)\n",
    "        \n",
    "        for t in range(len(states_and_rewards) - 1):\n",
    "            s1, r1 = states_and_rewards[t]\n",
    "            s2, r2 = states_and_rewards[t+1]\n",
    "            \n",
    "            old_theta = model.theta.copy()\n",
    "            if grid.is_terminal(s2):\n",
    "                target = r1\n",
    "            else:\n",
    "                target = r1 + GAMMA * model.predict(s2)\n",
    "            \n",
    "            model.theta += alpha*(target - model.predict(s1))*model.grad(s1)\n",
    "            max_change = max(max_change, np.abs(old_theta - model.theta).sum())\n",
    "            \n",
    "        deltas.append(max_change)\n",
    "        \n",
    "        \n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            V[s] = model.predict(s)\n",
    "        else:\n",
    "            V[s] = 0\n",
    "        \n",
    "    print(\"\\n\\nvalues\")\n",
    "    print_values(V, grid)\n",
    "    \n",
    "    print(\"\\n\\npolicy\")\n",
    "    print_policy(policy, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
